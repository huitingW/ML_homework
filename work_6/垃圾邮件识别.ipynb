{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085824ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from  sklearn import naive_bayes\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc04ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将整个邮件当成一个字符串处理，其中回车和换行需要过滤掉\n",
    "def load_one_file(filename):\n",
    "    x=\"\"\n",
    "    with open(filename,'r',encoding='latin-1') as f:  #因为一些邮件没有使用统一码，这条语句试图正确解码文件。\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            line=line.strip('\\r')\n",
    "            x+=line\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79d7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#遍历指定文件夹下所有文件，加载数据\n",
    "def load_files_from_dir(rootdir):\n",
    "    x=[]\n",
    "    list = os.listdir(rootdir)\n",
    "    for i in range(0, len(list)):\n",
    "        path = os.path.join(rootdir, list[i])\n",
    "        if os.path.isfile(path):\n",
    "            v=load_one_file(path)\n",
    "            x.append(v)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e261428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载所在的文件夹，正常邮件在ham中，垃圾邮件在spam中。\n",
    "\n",
    "datadir = './data_6'\n",
    "\n",
    "def load_all_files():\n",
    "    ham = []\n",
    "    spam = []\n",
    "    for i in range(1,7):\n",
    "        path = datadir + '/enron%d/ham/' %i\n",
    "        print(\"Load %s\"%path)\n",
    "        ham += load_files_from_dir(path)\n",
    "        \n",
    "        path = datadir + '/enron%d/spam/' %i\n",
    "        print(\"Load %s\"%path)\n",
    "        spam += load_files_from_dir(path)\n",
    "        \n",
    "    return ham, spam       ##正常文件：ham；垃圾邮件：spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0c2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用词袋模型，向量化邮件样本，ham标记为0，spam标记为1\n",
    "def get_features_by_wordbag():\n",
    "    ham, spam=load_all_files()\n",
    "    x=ham+spam\n",
    "    y=[0]*len(ham)+[1]*len(spam)\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore', #处理解码失败的方式\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print(vectorizer)\n",
    "    x=vectorizer.fit_transform(x)\n",
    "    x=x.toarray()\n",
    "    return x,y\n",
    "\n",
    "#使用词袋模型+TF-IDF\n",
    "def get_features_by_wordbag_tfidf():\n",
    "    ham, spam = load_all_files()\n",
    "    x = ham + spam\n",
    "    y = [0] * len(ham) + [1] * len(spam)\n",
    "    vectorizer = CountVectorizer(\n",
    "    decode_error = 'ignore',  #处理解码失败的方式\n",
    "    strip_accents = 'ascii',  #在预处理步骤中移除重音的方式\n",
    "    max_features = max_features, #词袋特征个数的最大值\n",
    "    stop_words = 'english',  #判断word结束的方式\n",
    "    max_df = 1.0,  #df最大值\n",
    "    min_df = 1,  #df最小值\n",
    "    binary = True) #默认为False，与TF-IDF结合时需要设置为True\n",
    "    x = vectorizer.fit_transform(x)\n",
    "    x = x.toarray()\n",
    "    transformer = TfidfTransformer(smooth_idf = False)\n",
    "    tfidf = transformer.fit_transform(x)\n",
    "    x = tfidf.toarray()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb345da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建贝叶斯模型\n",
    "def do_nb_wordbag(x_train, x_test, y_train, y_test):\n",
    "    print(\"NB and wordbag\")\n",
    "    gnb = naive_bayes.GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0755a183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello spam-mail\n",
      "get_features_by_wordbag_tfidf\n",
      "Load ./data_6/enron1/ham/\n",
      "Load ./data_6/enron1/spam/\n",
      "Load ./data_6/enron2/ham/\n",
      "Load ./data_6/enron2/spam/\n",
      "Load ./data_6/enron3/ham/\n",
      "Load ./data_6/enron3/spam/\n",
      "Load ./data_6/enron4/ham/\n",
      "Load ./data_6/enron4/spam/\n",
      "Load ./data_6/enron5/ham/\n",
      "Load ./data_6/enron5/spam/\n",
      "Load ./data_6/enron6/ham/\n",
      "Load ./data_6/enron6/spam/\n",
      "NB and wordbag\n",
      "0.9595195729537367\n",
      "[[3211   43]\n",
      " [ 230 3260]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    max_features=5000\n",
    "    max_document_length=100\n",
    "    \n",
    "    print(\"Hello spam-mail\")\n",
    "    #print(\"get_features_by_wordbag\")\n",
    "    #x,y=get_features_by_wordbag()\n",
    "    print(\"get_features_by_wordbag_tfidf\")\n",
    "    \n",
    "    x,y=get_features_by_wordbag_tfidf()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)  #测试集比例为20%\n",
    "    do_nb_wordbag(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8fd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e658a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
